{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb7d018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1563, -0.0939, -0.0995,  0.0680, -0.1852, -0.1102, -0.1033,  0.0302,\n",
       "          0.0159,  0.1139],\n",
       "        [-0.0949,  0.0053, -0.1337,  0.0597, -0.3144, -0.2417, -0.0579,  0.1460,\n",
       "          0.0222, -0.1633]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(20,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,10)\n",
    ")\n",
    "\n",
    "X = torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f277ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "    def forward(self,x):\n",
    "        return self.out(\n",
    "            nn.functional.relu(\n",
    "                self.hidden(x)\n",
    "                )\n",
    "                )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f3509b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1311, -0.1025, -0.0023,  0.4449,  0.0850, -0.0167, -0.0277, -0.1157,\n",
       "         -0.0028, -0.1713],\n",
       "        [ 0.2097, -0.1820,  0.0678,  0.5252,  0.3014,  0.2820, -0.1557, -0.0309,\n",
       "          0.1626, -0.0636]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "mlp(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abbcc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block] = block\n",
    "            #_modules存储每一个层次\n",
    "    def forward(self,x):\n",
    "        for block in self._modules:\n",
    "            x = self._modules[block](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfcd2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0314,  0.0086,  0.1921,  0.1685, -0.0268, -0.0832,  0.1739,  0.0237,\n",
       "          0.1123,  0.1330, -0.2368, -0.3567, -0.0277, -0.3713,  0.0514,  0.0868,\n",
       "          0.0530,  0.1382,  0.1842, -0.0025],\n",
       "        [-0.0207,  0.0738,  0.2415,  0.2903,  0.0483, -0.0629, -0.0120, -0.0575,\n",
       "          0.0378,  0.2715, -0.2435, -0.3688, -0.0015, -0.3860,  0.1626,  0.1011,\n",
       "         -0.0522, -0.0285, -0.0169,  0.0098]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySequential = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,20))\n",
    "mySequential(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec31b4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1176, 0.5253, 0.0810, 0.6533, 0.8439, 0.7275, 0.2293, 0.0317, 0.3611,\n",
      "         0.4429, 0.2449, 0.7859, 0.0780, 0.5571, 0.2351, 0.9392, 0.9817, 0.5971,\n",
      "         0.1370, 0.2063],\n",
      "        [0.7332, 0.8218, 0.9665, 0.6742, 0.5876, 0.3272, 0.4257, 0.9351, 0.9907,\n",
      "         0.2455, 0.6517, 0.8425, 0.1028, 0.1235, 0.4460, 0.7451, 0.6818, 0.7310,\n",
      "         0.9973, 0.0883],\n",
      "        [0.5233, 0.7091, 0.5605, 0.1077, 0.4590, 0.0365, 0.1171, 0.9115, 0.8605,\n",
      "         0.5638, 0.5301, 0.3768, 0.3272, 0.1396, 0.3287, 0.4059, 0.0131, 0.2461,\n",
      "         0.0068, 0.5053],\n",
      "        [0.9554, 0.1717, 0.0141, 0.7964, 0.9954, 0.2760, 0.0114, 0.2313, 0.4736,\n",
      "         0.2756, 0.7745, 0.5717, 0.9940, 0.0161, 0.8987, 0.1239, 0.3773, 0.2726,\n",
      "         0.7920, 0.8808],\n",
      "        [0.5553, 0.1954, 0.8601, 0.5409, 0.0624, 0.5248, 0.9617, 0.3059, 0.5369,\n",
      "         0.1424, 0.4110, 0.6561, 0.8370, 0.5233, 0.6737, 0.9737, 0.6507, 0.5511,\n",
      "         0.2186, 0.3120],\n",
      "        [0.1941, 0.0492, 0.4209, 0.7148, 0.1700, 0.3408, 0.1177, 0.7339, 0.0025,\n",
      "         0.1643, 0.9270, 0.1977, 0.5128, 0.1437, 0.1784, 0.2050, 0.5251, 0.2898,\n",
      "         0.4849, 0.8340],\n",
      "        [0.5160, 0.4306, 0.7649, 0.2695, 0.8001, 0.7306, 0.8637, 0.0840, 0.3847,\n",
      "         0.3391, 0.3745, 0.1930, 0.0159, 0.3688, 0.5281, 0.6874, 0.8046, 0.8177,\n",
      "         0.7640, 0.0032],\n",
      "        [0.8046, 0.0293, 0.8205, 0.7535, 0.7728, 0.4367, 0.7761, 0.3423, 0.1806,\n",
      "         0.2809, 0.6891, 0.0064, 0.5691, 0.8830, 0.9793, 0.4633, 0.0076, 0.5952,\n",
      "         0.2439, 0.5683],\n",
      "        [0.9928, 0.8308, 0.9008, 0.3695, 0.5998, 0.2173, 0.3644, 0.3811, 0.5962,\n",
      "         0.1393, 0.9248, 0.5334, 0.3698, 0.9403, 0.9689, 0.0224, 0.2387, 0.4477,\n",
      "         0.0703, 0.2582],\n",
      "        [0.7937, 0.5103, 0.2482, 0.9449, 0.9314, 0.1574, 0.0859, 0.9708, 0.9068,\n",
      "         0.3650, 0.4006, 0.8139, 0.6366, 0.5300, 0.2787, 0.7125, 0.8726, 0.2071,\n",
      "         0.5301, 0.1397],\n",
      "        [0.3713, 0.7542, 0.7347, 0.4765, 0.9392, 0.1361, 0.6660, 0.2447, 0.5745,\n",
      "         0.3706, 0.3298, 0.7946, 0.4162, 0.9933, 0.8098, 0.0952, 0.7462, 0.9276,\n",
      "         0.8519, 0.0323],\n",
      "        [0.8666, 0.9799, 0.7078, 0.8451, 0.3469, 0.6656, 0.0703, 0.2647, 0.4220,\n",
      "         0.1570, 0.4795, 0.2051, 0.4408, 0.1976, 0.6020, 0.3395, 0.3340, 0.3091,\n",
      "         0.8064, 0.7653],\n",
      "        [0.5368, 0.2350, 0.5523, 0.9348, 0.7009, 0.7647, 0.1900, 0.6305, 0.8900,\n",
      "         0.2771, 0.3751, 0.8138, 0.5090, 0.9922, 0.6190, 0.9658, 0.7005, 0.4916,\n",
      "         0.2271, 0.9296],\n",
      "        [0.6800, 0.0746, 0.6551, 0.1428, 0.8004, 0.2326, 0.5930, 0.0072, 0.8653,\n",
      "         0.6592, 0.3255, 0.2129, 0.0634, 0.8855, 0.2624, 0.7483, 0.7219, 0.1412,\n",
      "         0.5287, 0.4705],\n",
      "        [0.4703, 0.6161, 0.0716, 0.8408, 0.3868, 0.4956, 0.6315, 0.6128, 0.9687,\n",
      "         0.6562, 0.7955, 0.5760, 0.2849, 0.9668, 0.1342, 0.6306, 0.9897, 0.9322,\n",
      "         0.1838, 0.6693],\n",
      "        [0.4219, 0.7323, 0.6285, 0.6423, 0.6508, 0.8995, 0.4316, 0.3073, 0.1555,\n",
      "         0.5119, 0.6414, 0.7745, 0.9297, 0.8347, 0.2399, 0.3260, 0.9666, 0.6198,\n",
      "         0.0044, 0.1575],\n",
      "        [0.9649, 0.2297, 0.9008, 0.1855, 0.3072, 0.7518, 0.0213, 0.8123, 0.8423,\n",
      "         0.6339, 0.2137, 0.2555, 0.7315, 0.5934, 0.3089, 0.5181, 0.7389, 0.3775,\n",
      "         0.4984, 0.7704],\n",
      "        [0.1956, 0.1172, 0.6435, 0.6165, 0.2500, 0.0323, 0.3585, 0.4113, 0.6176,\n",
      "         0.0502, 0.9411, 0.6968, 0.4512, 0.7962, 0.6220, 0.4791, 0.7009, 0.7960,\n",
      "         0.1552, 0.6229],\n",
      "        [0.1005, 0.8374, 0.5293, 0.0779, 0.0483, 0.9439, 0.2161, 0.3609, 0.3117,\n",
      "         0.0308, 0.2928, 0.3826, 0.6411, 0.7616, 0.9264, 0.7346, 0.5798, 0.9753,\n",
      "         0.9434, 0.1336],\n",
      "        [0.2147, 0.5369, 0.0921, 0.7870, 0.5853, 0.1381, 0.7570, 0.1247, 0.9036,\n",
      "         0.7337, 0.9322, 0.7140, 0.2549, 0.2332, 0.8498, 0.9011, 0.0449, 0.3839,\n",
      "         0.2824, 0.3652]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.2711, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHidenMLP(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=False)\n",
    "        print(self.rand_weight)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        x = nn.functional.relu(torch.mm(x,self.rand_weight) + 1)\n",
    "        x = self.linear(x)\n",
    "        while x.abs().sum()>1:\n",
    "            x/=2\n",
    "        return x.sum()\n",
    "\n",
    "net = FixedHidenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1c523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1892],\n",
       "        [-0.4348]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "x = torch.rand((2,4))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9759a7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'0.weight': tensor([[-0.0144,  0.3360,  0.0708,  0.4889],\n",
      "        [-0.2393, -0.2710,  0.4229, -0.1168],\n",
      "        [ 0.1400, -0.0177,  0.3402, -0.0009],\n",
      "        [-0.3558, -0.1241,  0.1511, -0.1392],\n",
      "        [-0.2038,  0.2363,  0.1366,  0.3959],\n",
      "        [ 0.0528,  0.2139, -0.1727,  0.0112],\n",
      "        [ 0.1685, -0.1272, -0.2787, -0.4560],\n",
      "        [-0.0203,  0.4009,  0.1082, -0.4037]]), '0.bias': tensor([ 0.4968, -0.0687,  0.3989, -0.3411, -0.3010,  0.3217,  0.1634,  0.1873]), '2.weight': tensor([[-0.2831, -0.1837, -0.2420,  0.0758, -0.2390,  0.0609,  0.2733, -0.0329]]), '2.bias': tensor([0.1917])})\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6772ea97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name , param = net[0].named_parameters()\n",
    "type(net[0].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ff6081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
