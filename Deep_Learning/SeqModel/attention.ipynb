{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427f5b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef4bc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = 50\n",
    "x_train , _ = torch.sort(torch.rand(n_train)*5)\n",
    "def f(x):\n",
    "    return 2 * torch.sin(x) + x**0.08\n",
    "y_train = f(x_train) + torch.normal(0.0,0.5,(n_train,))\n",
    "\n",
    "x_text = torch.arange(0,5,0.1)\n",
    "y_truth = f(x_text)\n",
    "n_text = len(x_text)\n",
    "n_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db52b4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9687e-03, 1.8676e-01, 1.9078e-01, 2.6355e-01, 3.6642e-01, 4.1210e-01,\n",
       "        4.3030e-01, 5.1479e-01, 5.2907e-01, 9.1861e-01, 1.0331e+00, 1.1311e+00,\n",
       "        1.6257e+00, 1.8099e+00, 1.8337e+00, 1.9772e+00, 1.9796e+00, 1.9948e+00,\n",
       "        2.0027e+00, 2.0329e+00, 2.1162e+00, 2.1245e+00, 2.3119e+00, 2.3209e+00,\n",
       "        2.3820e+00, 2.6758e+00, 2.8519e+00, 2.8930e+00, 3.0164e+00, 3.0442e+00,\n",
       "        3.0700e+00, 3.3374e+00, 3.4612e+00, 3.4791e+00, 3.5098e+00, 3.5247e+00,\n",
       "        3.6088e+00, 3.6794e+00, 3.8105e+00, 3.9912e+00, 4.0013e+00, 4.0103e+00,\n",
       "        4.2490e+00, 4.2666e+00, 4.3892e+00, 4.8919e+00, 4.9307e+00, 4.9562e+00,\n",
       "        4.9864e+00, 4.9957e+00])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e69d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
       "        0.9000, 1.0000, 1.1000, 1.2000, 1.3000, 1.4000, 1.5000, 1.6000, 1.7000,\n",
       "        1.8000, 1.9000, 2.0000, 2.1000, 2.2000, 2.3000, 2.4000, 2.5000, 2.6000,\n",
       "        2.7000, 2.8000, 2.9000, 3.0000, 3.1000, 3.2000, 3.3000, 3.4000, 3.5000,\n",
       "        3.6000, 3.7000, 3.8000, 3.9000, 4.0000, 4.1000, 4.2000, 4.3000, 4.4000,\n",
       "        4.5000, 4.6000, 4.7000, 4.8000, 4.9000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a037323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20697/2577876924.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weights = nn.functional.softmax(-(X_repeat-x_train)**2/2)\n"
     ]
    }
   ],
   "source": [
    "X_repeat = x_text.repeat_interleave(n_train).reshape((-1,n_train))\n",
    "attention_weights = nn.functional.softmax(-(X_repeat-x_train)**2/2)\n",
    "y_hat = torch.matmul(attention_weights,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
