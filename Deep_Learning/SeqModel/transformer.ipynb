{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330a86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "def transpose_qkv(X,num_heads):\n",
    "    # batch_size,tokens,embed_nmum\n",
    "    X = X.reshape(X.shape[0],X.shape[1],num_heads,-1)\n",
    "    # batch_size,tokens,num_heads,embed_num/num_heads\n",
    "    X = X.permute(0,2,1,3)\n",
    "    # batch_size,num_heads,num_tokens,embed_num/num_heads\n",
    "    return X.reshape(-1,X.shape[2],X.shape[3])\n",
    "    # batch_size*num_heads ,num_tokens,embed_num/num_heads\n",
    "def transpose_output(X,num_heads):\n",
    "    # batch_size*num_heads ,num_tokens,embed_num/num_heads\n",
    "    X = X.reshape(-1,num_heads,X.shape[1],X.shape[2])\n",
    "    # batch_size,num_heads,num_tokens,embed/num_heads\n",
    "    X.permute(0,2,1,3)\n",
    "    # batch_size,num_tokens,num_head,embed/num_heads\n",
    "    return X.reshape(X.shape[0],X.shape[1],-1)\n",
    "    # batch_size,num_tokens,embed_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f5725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, key_size,\n",
    "                 query_size,\n",
    "                 value_size,\n",
    "                 num_hiddens,\n",
    "                 num_head,\n",
    "                 dropout,\n",
    "                 bias=False,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_head = num_head\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size,num_hiddens,bias=bias)\n",
    "        self.W_k = nn.Linear(key_size,num_hiddens,bias=bias)\n",
    "        self.W_v = nn.Linear(value_size,num_hiddens,bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens,num_hiddens,bias=bias)\n",
    "    \n",
    "    def forward(self,queries,keys,values,valid_lens):\n",
    "        queries = transpose_qkv(self.W_k(queries),self.num_head)\n",
    "        keys = transpose_qkv(self.W_k(keys),self.num_head)\n",
    "        values = transpose_qkv(self.W_q(values),self.num_head)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens,\n",
    "                                                 repeats=self.num_head,\n",
    "                                                 dim=0)\n",
    "        output = self.attention(queries,keys,values,valid_lens)\n",
    "\n",
    "        output_concat = transpose_output(output,self.num_head)\n",
    "\n",
    "        return self.W_o(output_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32e0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self,ffn_num_input,ffn_num_hiddens,ffn_num_output,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input,ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens,ffn_num_output)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb103c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self,normalized_shape,droupt, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.dropout = nn.Dropout(droupt)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self,X,Y):\n",
    "        return self.ln(self.dropout(Y),X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97094f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,key_size,\n",
    "                 query_size,\n",
    "                 values_size,\n",
    "                 num_hidden,\n",
    "                 norm_shape,\n",
    "                 ffn_num_input,\n",
    "                 ffn_num_hidden,\n",
    "                 num_head,\n",
    "                dropout,\n",
    "                use_bias=False,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.attention = MultiHeadAttention(key_size,query_size,values_size,num_head,num_head,dropout,use_bias)\n",
    "\n",
    "        self.addnorml1 = AddNorm(normalized_shape=norm_shape,droupt=dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input=ffn_num_input,\n",
    "                                   ffn_num_hiddens=ffn_num_hidden,\n",
    "                                   ffn_num_output=num_hidden)\n",
    "        self.addnorml2 = AddNorm(norm_shape,dropout)\n",
    "    def forward(self,X,valid_len):\n",
    "        Y = self.addnorml1(X,self.attention(X,X,X,valid_len))\n",
    "        return self.addnorml2(Y,self.ffn(Y))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "198aa742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(d2l.Encoder):\n",
    "    def __init__(self,vocab_size,\n",
    "                 ket_size,\n",
    "                 query_size,\n",
    "                 value_size,\n",
    "                 num_hiddens,\n",
    "                 norm_shape,\n",
    "                 ffn_num_input,\n",
    "                 ffn_num_hidden,\n",
    "                 num_head,num_layer,droupt,use_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens=num_hiddens,dropout=droupt)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layer):\n",
    "            self.add_module(\n",
    "                \"block\"+str(i),\n",
    "                EncoderBlock(\n",
    "                    key_size=ket_size,query_size=query_size,\n",
    "                    values_size=value_size,\n",
    "                    num_hidden=num_hiddens,\n",
    "                    norm_shape=norm_shape,\n",
    "                    ffn_num_hidden=ffn_num_hidden,ffn_num_input=ffn_num_input,\n",
    "                    num_head=num_head,\n",
    "                    dropout=droupt,use_bias=use_bias\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, X,valid_lens,*agrs):\n",
    "        X = self.pos_encoding(\n",
    "            self.embedding(X) * math.sqrt(self.num_hiddens)\n",
    "        )\n",
    "        self.attention_weights = [None]*len(self.blks)\n",
    "        for i,blk in enumerate(self.blks):\n",
    "            X = blk(X,valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7581c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, key_size,\n",
    "                 query_size,\n",
    "                 value_size,\n",
    "                 num_hidden,\n",
    "                 norm_shape,\n",
    "                 ffn_num_input,\n",
    "                 ffn_num_hidden,\n",
    "                 num_head,\n",
    "                 dropout,\n",
    "                 i,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.i = i\n",
    "        self.attention1 = MultiHeadAttention(key_size,query_size,value_size,num_head,num_head,dropout)\n",
    "\n",
    "        self.addnorm1 = d2l.AddNorm(norm_shape,dropout)\n",
    "\n",
    "        self.attention2 = MultiHeadAttention(key_size,query_size,value_size,num_head,num_head,dropout)\n",
    "        self.addnorm2 = d2l.AddNorm(norm_shape,dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input,ffn_num_hidden,num_hidden)\n",
    "\n",
    "        self.addnorm3 = d2l.AddNorm(norm_shape,dropout)\n",
    "    \n",
    "    def forward(self,X,state):\n",
    "        enc_outputs ,enc_valid_lens = state[0],state[1]\n",
    "        \n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i],X),dim=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:\n",
    "           batch_size,num_step,_ = X.shape\n",
    "           dec_valid_lens = torch.arange(1,num_step+1,device=X.device).repeat(\n",
    "               batch_size,1\n",
    "           ) \n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "        \n",
    "        X2 = self.attention1(X,key_values,key_values,dec_valid_lens)\n",
    "        Y = self.addnorm1(X,X2)\n",
    "        Y2 = self.attention2(Y,enc_outputs,enc_outputs,enc_valid_lens)\n",
    "        z = self.addnorm2(Y,Y2)\n",
    "        return self.addnorm3(z,self.ffn(z)),state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "481ffab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 key_size,\n",
    "                 query_size,\n",
    "                 value_size,\n",
    "                 num_hiddens,\n",
    "                 norm_shape,\n",
    "                 ffn_num_input,\n",
    "                 ffn_num_hidden,\n",
    "                 num_head,\n",
    "                 num_layer,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layer = num_layer\n",
    "        self.embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens,dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layer):\n",
    "            self.blks.add_module(\n",
    "                \"block\"+str(i),\n",
    "                DecoderBlock(\n",
    "                    key_size=key_size,\n",
    "                    query_size=query_size,\n",
    "                    value_size=value_size,\n",
    "                    num_hidden=num_hiddens,\n",
    "                    norm_shape=norm_shape,\n",
    "                    ffn_num_input=ffn_num_input,\n",
    "                    ffn_num_hidden=ffn_num_hidden,\n",
    "                    num_head=num_head,\n",
    "                    dropout=dropout,\n",
    "                    i=i\n",
    "                )\n",
    "            )\n",
    "        self.dense = nn.Linear(num_hiddens,vocab_size)\n",
    "    def init_state(self,enc_outputs,enc_valid_lens):\n",
    "        return [enc_outputs,enc_valid_lens,None]\n",
    "    def forward(self,X,state):\n",
    "        X = self.pos_encoding(X)\n",
    "        self._attention_weights = [[None]*len(self.blks) for _ in range(2)]\n",
    "\n",
    "        for i , blk in enumerate(self.blks):\n",
    "            X, state = blk(X,state)\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "            \n",
    "        return self.dense(X),state\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoGluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
