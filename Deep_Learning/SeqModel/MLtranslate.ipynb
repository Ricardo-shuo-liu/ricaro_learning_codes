{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d4123e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "import tarfile\n",
    "import zipfile\n",
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "\n",
    "DATA_HUB['fra-eng'] = (DATA_URL + 'fra-eng.zip',\n",
    "                           '94646ad1522d915e7b0f9296181140edcf86a4f5')\n",
    "\n",
    "def download(name, cache_dir=os.path.join('..', 'data')):  #@save\n",
    "    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} 不存在于 {DATA_HUB}\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # 命中缓存\n",
    "    print(f'正在从{url}下载{fname}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "def download_extract(name, folder=None):  #@save\n",
    "    \"\"\"下载并解压zip/tar文件\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == '.zip':\n",
    "        fp = zipfile.ZipFile(fname, 'r')\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        fp = tarfile.open(fname, 'r')\n",
    "    else:\n",
    "        assert False, '只有zip/tar文件可以被解压缩'\n",
    "    fp.extractall(base_dir)\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\n",
    "\n",
    "def read_data_nmt():\n",
    "    data_dir = download_extract('fra-eng')\n",
    "    with open(os.path.join(data_dir,'fra.txt'),'r') as f:\n",
    "        return f.read()\n",
    "raw_text = read_data_nmt()\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd58110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\tça alors !\n"
     ]
    }
   ],
   "source": [
    "def preprocess_nmt(text):\n",
    "    def no_space(char,prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "    text = text.replace('\\u202f',' ').replace('\\xa0',' ').lower()\n",
    "\n",
    "    out = [\n",
    "        ' ' + char if i > 0 and no_space(char,text[i-1]) else char\n",
    "        for i,char in enumerate(text)\n",
    "    ]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "075b022e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['go', '.'],\n",
       "  ['hi', '.'],\n",
       "  ['run', '!'],\n",
       "  ['run', '!'],\n",
       "  ['who', '?'],\n",
       "  ['wow', '!']],\n",
       " [['va', '!'],\n",
       "  ['salut', '!'],\n",
       "  ['cours', '!'],\n",
       "  ['courez', '!'],\n",
       "  ['qui', '?'],\n",
       "  ['ça', 'alors', '!']])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_nmt(text,num_examples=None):\n",
    "    source ,target = [],[]\n",
    "    for i,line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source,target\n",
    "\n",
    "source,target = tokenize_nmt(text)\n",
    "source[:6],target[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5347792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10012"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self,tokens=None,min_freq=0,reserved_token=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_token is None:\n",
    "            reserved_token = []\n",
    "        counter = self.count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(),key=lambda x :x[1],reverse=True)\n",
    "        \n",
    "        self.unk , uniq_token = 0,['<unk>'] + reserved_token\n",
    "\n",
    "        uniq_token+=[\n",
    "            token for token,freq in self.token_freqs\n",
    "            if freq >= min_freq and token not in uniq_token\n",
    "        ] \n",
    "        self.idx_to_token ,self.token_to_idx = [],dict()\n",
    "        for token in uniq_token:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "    def count_corpus(self,tokens):\n",
    "        if len(tokens)==0 or isinstance(tokens[0],list):\n",
    "            tokens = [token for line in tokens\n",
    "                      for token in line]\n",
    "        return collections.Counter(tokens)\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,indices):\n",
    "        if not isinstance(indices,(list,tuple)):\n",
    "            return self.token_to_idx[indices]\n",
    "        return [self.token_to_idx[index] for index in indices]\n",
    "    def to_tokens(self,indices):\n",
    "        if not isinstance(indices,(list,tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "src_vocab = Vocab(source,min_freq=2,reserved_token=['<pad>','<bos>','<eos>'])\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c912780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def truncate_pad(line,num_steps,padding_token):\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]\n",
    "    return line + [padding_token] * (num_steps - len(line))\n",
    "truncate_pad(src_vocab[source[0]],10,src_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f96d080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "def build_array_nmt(lines,vocab,num_steps):\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor(\n",
    "        [truncate_pad(l,num_steps,padding_token=vocab[\"<pad>\"]) for l in lines]\n",
    "    )\n",
    "    valid_len = (array !=vocab['<pad>']).type(torch.float32).sum(1)\n",
    "    return array ,valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebc687a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a048f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size,num_step,num_examples=600):\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source,target = tokenize_nmt(text,num_examples)\n",
    "    src_vocab = Vocab(source,min_freq=0,reserved_token=['<pad>','<bos>','<eos>'])\n",
    "    tgt_vocab = Vocab(target,min_freq=0,reserved_token=['<pad>','<bos>','<eos>'])\n",
    "\n",
    "    src_array ,src_valid_len = build_array_nmt(\n",
    "        source,\n",
    "        src_vocab,\n",
    "        num_steps=num_step\n",
    "    )\n",
    "    tgt_array,tgt_valid_len = build_array_nmt(\n",
    "        target,\n",
    "        tgt_vocab,\n",
    "        num_step\n",
    "    )\n",
    "    data_arrays = (src_array,src_valid_len,tgt_array,tgt_valid_len)\n",
    "    data_iter = load_array(data_arrays,batch_size)\n",
    "    return data_iter,src_vocab,tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0279e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 68,  60,   4,   3,   1,   1,   1,   1],\n",
      "        [  6, 197,   4,   3,   1,   1,   1,   1]])\n",
      "X.valid : tensor([4., 4.])\n",
      "Y: tensor([[ 64,  53,   4,   3,   1,   1,   1,   1],\n",
      "        [  6, 280,   4,   3,   1,   1,   1,   1]])\n",
      "Y.valid: tensor([4., 4.])\n"
     ]
    }
   ],
   "source": [
    "train_itrer,src_vocab,tgt_vocab =  load_data_nmt(batch_size=2,num_step=8)\n",
    "\n",
    "for X,X_valid_len ,Y,Y_valid_len in train_itrer:\n",
    "    print(\"X:\",X)\n",
    "    print(\"X.valid :\",X_valid_len)\n",
    "    print(\"Y:\",Y)\n",
    "    print(\"Y.valid:\",Y_valid_len)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
