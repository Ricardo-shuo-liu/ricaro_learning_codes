{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a20989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d14df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_and_segments(tokens_a,tokens_b=None):\n",
    "    tokens = ['<cls>'] + tokens_a + ['<seq>']\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments +=[1]*(len(tokens_b) + 1)\n",
    "    return tokens,segments\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "def transpose_qkv(X,num_heads):\n",
    "    # batch_size,tokens,embed_nmum\n",
    "    X = X.reshape(X.shape[0],X.shape[1],num_heads,-1)\n",
    "    # batch_size,tokens,num_heads,embed_num/num_heads\n",
    "    X = X.permute(0,2,1,3)\n",
    "    # batch_size,num_heads,num_tokens,embed_num/num_heads\n",
    "    return X.reshape(-1,X.shape[2],X.shape[3])\n",
    "    # batch_size*num_heads ,num_tokens,embed_num/num_heads\n",
    "def transpose_output(X,num_heads):\n",
    "    # batch_size*num_heads ,num_tokens,embed_num/num_heads\n",
    "    X = X.reshape(-1,num_heads,X.shape[1],X.shape[2])\n",
    "    # batch_size,num_heads,num_tokens,embed/num_heads\n",
    "    X.permute(0,2,1,3)\n",
    "    # batch_size,num_tokens,num_head,embed/num_heads\n",
    "    return X.reshape(X.shape[0],X.shape[1],-1)\n",
    "    # batch_size,num_tokens,embed_num\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, key_size,\n",
    "                 query_size,\n",
    "                 value_size,\n",
    "                 num_hiddens,\n",
    "                 num_head,\n",
    "                 dropout,\n",
    "                 bias=False,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_head = num_head\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size,num_hiddens,bias=bias)\n",
    "        self.W_k = nn.Linear(key_size,num_hiddens,bias=bias)\n",
    "        self.W_v = nn.Linear(value_size,num_hiddens,bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens,num_hiddens,bias=bias)\n",
    "    \n",
    "    def forward(self,queries,keys,values,valid_lens):\n",
    "        queries = transpose_qkv(self.W_k(queries),self.num_head)\n",
    "        keys = transpose_qkv(self.W_k(keys),self.num_head)\n",
    "        values = transpose_qkv(self.W_q(values),self.num_head)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens,\n",
    "                                                 repeats=self.num_head,\n",
    "                                                 dim=0)\n",
    "        output = self.attention(queries,keys,values,valid_lens)\n",
    "\n",
    "        output_concat = transpose_output(output,self.num_head)\n",
    "\n",
    "        return self.W_o(output_concat)\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self,normalized_shape,droupt, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.dropout = nn.Dropout(droupt)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self,X,Y):\n",
    "        return self.ln(self.dropout(Y),X)\n",
    "import pandas as pd\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self,ffn_num_input,ffn_num_hiddens,ffn_num_output,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input,ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens,ffn_num_output)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,key_size,\n",
    "                 query_size,\n",
    "                 values_size,\n",
    "                 num_hidden,\n",
    "                 norm_shape,\n",
    "                 ffn_num_input,\n",
    "                 ffn_num_hidden,\n",
    "                 num_head,\n",
    "                dropout,\n",
    "                use_bias=False,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.attention = MultiHeadAttention(key_size,query_size,values_size,num_head,num_head,dropout,use_bias)\n",
    "\n",
    "        self.addnorml1 = AddNorm(normalized_shape=norm_shape,droupt=dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input=ffn_num_input,\n",
    "                                   ffn_num_hiddens=ffn_num_hidden,\n",
    "                                   ffn_num_output=num_hidden)\n",
    "        self.addnorml2 = AddNorm(norm_shape,dropout)\n",
    "    def forward(self,X,valid_len):\n",
    "        Y = self.addnorml1(X,self.attention(X,X,X,valid_len))\n",
    "        return self.addnorml2(Y,self.ffn(Y))\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self,vocab_size,\n",
    "    num_hidden,\n",
    "    norm_shape,\n",
    "    ffn_num_input,\n",
    "    ffn_num_hidden,\n",
    "    num_heads,\n",
    "    num_layers,dropout,\n",
    "    max_len=1000,\n",
    "    key_size=768,\n",
    "    query_size=768,\n",
    "    value_size=768):\n",
    "        self.token_embedding = nn.Embedding(vocab_size,num_heads)\n",
    "        self.segment_embedding = nn.Embedding(2,num_hidden)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\n",
    "                f\"{i}\",\n",
    "                EncoderBlock(key_size=key_size,\n",
    "                             query_size=query_size,\n",
    "                             values_size=value_size,\n",
    "                             num_hidden=num_hidden,\n",
    "                             norm_shape=norm_shape,\n",
    "                             ffn_num_input=ffn_num_input,\n",
    "                             ffn_num_hidden=ffn_num_hidden,\n",
    "                             num_head=num_heads,\n",
    "                             dropout=dropout)\n",
    "            )\n",
    "        self.pos_embedding = nn.Parameter(torch.rand(1,max_len,num_hidden))\n",
    "    def forward(self,tokens,segments,valid_lens):\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding[:,:X.shape[1],:]\n",
    "\n",
    "        for blk in self.blks:\n",
    "            X = blk(X,valid_lens)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66859a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 num_hiddens,\n",
    "                 num_inputs=768,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_inputs,num_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(num_hiddens),\n",
    "            nn.Linear(num_hiddens,vocab_size)\n",
    "        )\n",
    "    def forward(self,X,pred_position):\n",
    "        # 1. 每个样本的掩码位置数量：3\n",
    "        num_pred_positions = pred_position.shape[1]  # 结果=3\n",
    "        # 2. 把位置展平：[[5,8,9],[2,4,7]] → [5,8,9,2,4,7]（shape [6]）\n",
    "        pred_position = pred_position.reshape(-1)  \n",
    "        # 3. 批次大小：2\n",
    "        batch_size = X.shape[0]  # 结果=2\n",
    "        # 4. 生成批次基础索引：[0,1]（shape [2]）\n",
    "        batch_idx = torch.arange(0,batch_size)  \n",
    "        # 5. 重复批次索引：[0,1] → [0,0,0,1,1,1]（每个索引重复3次，shape [6]）\n",
    "        batch_idx = torch.repeat_interleave(batch_idx,num_pred_positions)  \n",
    "        # 6. 向量化索引：取出X[0,5]、X[0,8]、X[0,9]、X[1,2]、X[1,4]、X[1,7]\n",
    "        # 结果shape [6,768]\n",
    "        masked_X = X[batch_idx,pred_position]  \n",
    "        # 7. 重塑回批次维度：[6,768] → [2,3,768]（2个样本，每个3个掩码位置，特征768）\n",
    "        masked_X = masked_X.reshape((batch_size,num_pred_positions,-1))  \n",
    "        # 8. MLP预测：[2,3,768] → [2,3,vocab_size]（每个掩码位置预测词汇表概率）\n",
    "        mlm_Y_hat = self.mlp(masked_X)  \n",
    "        return mlm_Y_hat\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5442c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    def __init__(self, num_inputs,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output = nn.Linear(num_inputs,2)\n",
    "    def forward(self,X):\n",
    "        return self.output(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328363aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_hiddens,\n",
    "                 norm_shape,\n",
    "                 ffn_num_inputs,\n",
    "                 ffn_num_hiddens,\n",
    "                 num_heads,\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 max_len=1000,\n",
    "                 key_size=768,\n",
    "                query_size=768,\n",
    "                value_size=768,\n",
    "                hid_in_features=768,\n",
    "                mlm_in_features=768,\n",
    "                nsp_in_features=768,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.encoder = BertEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            num_hidden=num_hiddens,\n",
    "            norm_shape=norm_shape,\n",
    "            ffn_num_input=ffn_num_inputs,\n",
    "            ffn_num_hidden=ffn_num_hiddens,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "            key_size=key_size,\n",
    "            query_size=query_size,\n",
    "            value_size=value_size\n",
    "        )\n",
    "        self.mlm = MaskLM(vocab_size=vocab_size,\n",
    "                          num_hiddens=num_hiddens,\n",
    "                          num_inputs=mlm_in_features)\n",
    "        self.nsp = NextSentencePred(num_inputs=nsp_in_features)\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(hid_in_features,num_hiddens),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,tokens,segments,valid_lens=None,pred_position=None):\n",
    "        encoded_X = self.encoder(tokens,segments,valid_lens)\n",
    "        if pred_position is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X,pred_position)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        nsp_Y_hat = self.nsp(self.hidden(mlm_Y_hat[:,0,:]))\n",
    "        return encoded_X,mlm_Y_hat,nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3beeffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f427a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "#@save\n",
    "def _read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # 大写字母转换为小写字母\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "\n",
    "def _get_next_sentence(sentenct,next_sentence,paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentenct,next_sentence,is_next\n",
    "\n",
    "def _get_nsp_data_from_paragraph(paragraph,paragraphs,vocab,max_len):\n",
    "    nsp_Data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        token_a,token_b ,is_next = _get_next_sentence(\n",
    "            paragraph[i],\n",
    "            paragraph[i + 1],\n",
    "            paragraphs\n",
    "        )\n",
    "        if len(token_a) + len(token_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens,seqments = d2l.get_tokens_and_segments(token_a,token_b)\n",
    "        nsp_Data_from_paragraph.append((tokens,seqments,is_next))\n",
    "    return nsp_Data_from_paragraph\n",
    "def _replace_mlm_tokens(tokens,\n",
    "                        candidate_pred_positions,\n",
    "                        num_mlm_preds,\n",
    "                        vocab):\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_label = []\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_label) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_label.append(\n",
    "            (mlm_pred_position,tokens[mlm_pred_position])\n",
    "        )\n",
    "    return mlm_input_tokens,pred_positions_and_label\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoGluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
