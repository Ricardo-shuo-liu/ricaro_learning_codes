{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77a20989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d14df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_and_segments(tokens_a,tokens_b=None):\n",
    "    tokens = ['<cls>'] + tokens_a + ['<seq>']\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments +=[1]*(len(tokens_b) + 1)\n",
    "    return tokens,segments\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "def transpose_qkv(X,num_heads):\n",
    "    # batch_size,tokens,embed_nmum\n",
    "    X = X.reshape(X.shape[0],X.shape[1],num_heads,-1)\n",
    "    # batch_size,tokens,num_heads,embed_num/num_heads\n",
    "    X = X.permute(0,2,1,3)\n",
    "    # batch_size,num_heads,num_tokens,embed_num/num_heads\n",
    "    return X.reshape(-1,X.shape[2],X.shape[3])\n",
    "    # batch_size*num_heads ,num_tokens,embed_num/num_heads\n",
    "def transpose_output(X,num_heads):\n",
    "    # batch_size*num_heads ,num_tokens,embed_num/num_heads\n",
    "    X = X.reshape(-1,num_heads,X.shape[1],X.shape[2])\n",
    "    # batch_size,num_heads,num_tokens,embed/num_heads\n",
    "    X.permute(0,2,1,3)\n",
    "    # batch_size,num_tokens,num_head,embed/num_heads\n",
    "    return X.reshape(X.shape[0],X.shape[1],-1)\n",
    "    # batch_size,num_tokens,embed_num\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, key_size,\n",
    "                 query_size,\n",
    "                 value_size,\n",
    "                 num_hiddens,\n",
    "                 num_head,\n",
    "                 dropout,\n",
    "                 bias=False,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_head = num_head\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size,num_hiddens,bias=bias)\n",
    "        self.W_k = nn.Linear(key_size,num_hiddens,bias=bias)\n",
    "        self.W_v = nn.Linear(value_size,num_hiddens,bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens,num_hiddens,bias=bias)\n",
    "    \n",
    "    def forward(self,queries,keys,values,valid_lens):\n",
    "        queries = transpose_qkv(self.W_k(queries),self.num_head)\n",
    "        keys = transpose_qkv(self.W_k(keys),self.num_head)\n",
    "        values = transpose_qkv(self.W_q(values),self.num_head)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens,\n",
    "                                                 repeats=self.num_head,\n",
    "                                                 dim=0)\n",
    "        output = self.attention(queries,keys,values,valid_lens)\n",
    "\n",
    "        output_concat = transpose_output(output,self.num_head)\n",
    "\n",
    "        return self.W_o(output_concat)\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self,normalized_shape,droupt, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.dropout = nn.Dropout(droupt)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self,X,Y):\n",
    "        return self.ln(self.dropout(Y),X)\n",
    "import pandas as pd\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self,ffn_num_input,ffn_num_hiddens,ffn_num_output,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input,ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens,ffn_num_output)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,key_size,\n",
    "                 query_size,\n",
    "                 values_size,\n",
    "                 num_hidden,\n",
    "                 norm_shape,\n",
    "                 ffn_num_input,\n",
    "                 ffn_num_hidden,\n",
    "                 num_head,\n",
    "                dropout,\n",
    "                use_bias=False,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.attention = MultiHeadAttention(key_size,query_size,values_size,num_head,num_head,dropout,use_bias)\n",
    "\n",
    "        self.addnorml1 = AddNorm(normalized_shape=norm_shape,droupt=dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input=ffn_num_input,\n",
    "                                   ffn_num_hiddens=ffn_num_hidden,\n",
    "                                   ffn_num_output=num_hidden)\n",
    "        self.addnorml2 = AddNorm(norm_shape,dropout)\n",
    "    def forward(self,X,valid_len):\n",
    "        Y = self.addnorml1(X,self.attention(X,X,X,valid_len))\n",
    "        return self.addnorml2(Y,self.ffn(Y))\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self,vocab_size,\n",
    "    num_hidden,\n",
    "    norm_shape,\n",
    "    ffn_num_input,\n",
    "    ffn_num_hidden,\n",
    "    num_heads,\n",
    "    num_layers,dropout,\n",
    "    max_len=1000,\n",
    "    key_size=768,\n",
    "    query_size=768,\n",
    "    value_size=768):\n",
    "        self.token_embedding = nn.Embedding(vocab_size,num_heads)\n",
    "        self.segment_embedding = nn.Embedding(2,num_hidden)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\n",
    "                f\"{i}\",\n",
    "                EncoderBlock(key_size=key_size,\n",
    "                             query_size=query_size,\n",
    "                             values_size=value_size,\n",
    "                             num_hidden=num_hidden,\n",
    "                             norm_shape=norm_shape,\n",
    "                             ffn_num_input=ffn_num_input,\n",
    "                             ffn_num_hidden=ffn_num_hidden,\n",
    "                             num_head=num_heads,\n",
    "                             dropout=dropout)\n",
    "            )\n",
    "        self.pos_embedding = nn.Parameter(torch.rand(1,max_len,num_hidden))\n",
    "    def forward(self,tokens,segments,valid_lens):\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding[:,:X.shape[1],:]\n",
    "\n",
    "        for blk in self.blks:\n",
    "            X = blk(X,valid_lens)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66859a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 num_hiddens,\n",
    "                 num_inputs=768,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_inputs,num_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(num_hiddens),\n",
    "            nn.Linear(num_hiddens,vocab_size)\n",
    "        )\n",
    "    def forward(self,X,pred_position):\n",
    "        # 1. 每个样本的掩码位置数量：3\n",
    "        num_pred_positions = pred_position.shape[1]  # 结果=3\n",
    "        # 2. 把位置展平：[[5,8,9],[2,4,7]] → [5,8,9,2,4,7]（shape [6]）\n",
    "        pred_position = pred_position.reshape(-1)  \n",
    "        # 3. 批次大小：2\n",
    "        batch_size = X.shape[0]  # 结果=2\n",
    "        # 4. 生成批次基础索引：[0,1]（shape [2]）\n",
    "        batch_idx = torch.arange(0,batch_size)  \n",
    "        # 5. 重复批次索引：[0,1] → [0,0,0,1,1,1]（每个索引重复3次，shape [6]）\n",
    "        batch_idx = torch.repeat_interleave(batch_idx,num_pred_positions)  \n",
    "        # 6. 向量化索引：取出X[0,5]、X[0,8]、X[0,9]、X[1,2]、X[1,4]、X[1,7]\n",
    "        # 结果shape [6,768]\n",
    "        masked_X = X[batch_idx,pred_position]  \n",
    "        # 7. 重塑回批次维度：[6,768] → [2,3,768]（2个样本，每个3个掩码位置，特征768）\n",
    "        masked_X = masked_X.reshape((batch_size,num_pred_positions,-1))  \n",
    "        # 8. MLP预测：[2,3,768] → [2,3,vocab_size]（每个掩码位置预测词汇表概率）\n",
    "        mlm_Y_hat = self.mlp(masked_X)  \n",
    "        return mlm_Y_hat\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e5442c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    def __init__(self, num_inputs,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output = nn.Linear(num_inputs,2)\n",
    "    def forward(self,X):\n",
    "        return self.output(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "328363aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_hiddens,\n",
    "                 norm_shape,\n",
    "                 ffn_num_inputs,\n",
    "                 ffn_num_hiddens,\n",
    "                 num_heads,\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 max_len=1000,\n",
    "                 key_size=768,\n",
    "                query_size=768,\n",
    "                value_size=768,\n",
    "                hid_in_features=768,\n",
    "                mlm_in_features=768,\n",
    "                nsp_in_features=768,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.encoder = BertEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            num_hidden=num_hiddens,\n",
    "            norm_shape=norm_shape,\n",
    "            ffn_num_input=ffn_num_inputs,\n",
    "            ffn_num_hidden=ffn_num_hiddens,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "            key_size=key_size,\n",
    "            query_size=query_size,\n",
    "            value_size=value_size\n",
    "        )\n",
    "        self.mlm = MaskLM(vocab_size=vocab_size,\n",
    "                          num_hiddens=num_hiddens,\n",
    "                          num_inputs=mlm_in_features)\n",
    "        self.nsp = NextSentencePred(num_inputs=nsp_in_features)\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(hid_in_features,num_hiddens),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,tokens,segments,valid_lens=None,pred_position=None):\n",
    "        encoded_X = self.encoder(tokens,segments,valid_lens)\n",
    "        if pred_position is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X,pred_position)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        nsp_Y_hat = self.nsp(self.hidden(mlm_Y_hat[:,0,:]))\n",
    "        return encoded_X,mlm_Y_hat,nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3beeffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f427a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "#@save\n",
    "def _read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # 大写字母转换为小写字母\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "\n",
    "def _get_next_sentence(sentenct,next_sentence,paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentenct,next_sentence,is_next\n",
    "\n",
    "def _get_nsp_data_from_paragraph(paragraph,paragraphs,vocab,max_len):\n",
    "    nsp_Data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        token_a,token_b ,is_next = _get_next_sentence(\n",
    "            paragraph[i],\n",
    "            paragraph[i + 1],\n",
    "            paragraphs\n",
    "        )\n",
    "        if len(token_a) + len(token_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens,seqments = d2l.get_tokens_and_segments(token_a,token_b)\n",
    "        nsp_Data_from_paragraph.append((tokens,seqments,is_next))\n",
    "    return nsp_Data_from_paragraph\n",
    "def _replace_mlm_tokens(tokens,\n",
    "                        candidate_pred_positions,\n",
    "                        num_mlm_preds,\n",
    "                        vocab):\n",
    "    # 复制原始token序列（避免修改原列表）\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    # 用于存储：(掩码位置, 原始token)，后续作为MLM的预测位置和标签\n",
    "    pred_positions_and_label = []\n",
    "    # 打乱候选掩码位置（随机选择要掩码的位置）\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        # 控制掩码数量：达到目标数量就停止\n",
    "        if len(pred_positions_and_label) >= num_mlm_preds:\n",
    "            break   \n",
    "        masked_token = None\n",
    "        # 规则1：80% 的概率替换为 <mask> 符号\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 规则2：剩下20% 中，50% 保留原词（即10% 总概率）\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 规则3：剩下50% 随机替换为词汇表中的其他词（即10% 总概率）\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        # 用生成的masked_token替换原序列中的对应位置\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        # 记录：掩码位置 + 原始token（后续用来计算MLM损失）\n",
    "        pred_positions_and_label.append(\n",
    "            (mlm_pred_position,tokens[mlm_pred_position])\n",
    "        )\n",
    "    return mlm_input_tokens,pred_positions_and_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e1e8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mlm_data_from_tokens(tokens,vocab):\n",
    "    candidate_pred_position = []\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token in [\"<cls>\",\"<sep>\"]:\n",
    "            continue\n",
    "        candidate_pred_position.append(i)\n",
    "    num_mlm_preds = max(1,round(len(tokens) * 0.15))\n",
    "    # 0.15 mask round 取整数\n",
    "    mlm_input_tokens,pred_positions_and_label = _replace_mlm_tokens(tokens,\n",
    "                                                                    candidate_pred_position,\n",
    "                                                                    num_mlm_preds,\n",
    "                                                                    vocab)\n",
    "    pred_positions_and_label = sorted(pred_positions_and_label,key=lambda x:x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_label]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_label]\n",
    "    return vocab[mlm_input_tokens],pred_positions,vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d80c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_bert_inputs(examples,max_len,vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids,all_segments,valid_lens = [],[],[]\n",
    "    all_pred_positions,all_mlm_weights,all_mlm_labels = [],[],[]\n",
    "    nsp_labels = []\n",
    "    for token_ids,pred_positions,mlm_pred_label_ids,segments,is_next in examples:\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab[\"<pad>\"]] * (max_len - len(token_ids)),\n",
    "                                          dtype=torch.float32))\n",
    "        all_segments.append(torch.tensor(segments + [0] *(max_len - len(segments))),dtype=torch.long)\n",
    "\n",
    "        valid_lens.append(torch.tensor(len(token_ids),dtype=torch.float32))\n",
    "\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] *(max_num_mlm_preds - len(pred_positions))))\n",
    "\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor(\n",
    "            [1.0] * len(mlm_pred_label_ids) + [0.0]*(max_num_mlm_preds - len(pred_positions))\n",
    "        ,dtype=torch.float32))\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0]*(max_num_mlm_preds - len(mlm_pred_label_ids)),dtype=torch.float32))\n",
    "        \n",
    "        nsp_labels.append(torch.tensor(is_next,dtype=torch.long))\n",
    "\n",
    "    return all_token_ids,all_segments,valid_lens,all_pred_positions,all_mlm_weights,all_mlm_labels,nsp_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a919e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,paragraphs,max_len):\n",
    "        super().__init__()\n",
    "        paragraphs = [d2l.tokenize(\n",
    "            paragraph,token=\"word\"\n",
    "        ) for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "\n",
    "        self.vocab = d2l.Vocab(sentences,min_freq=5,reserved_tokens=[\"<pad>\",\"<mask>\",\"<cls>\",\"<sep\"])\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(paragraph=paragraph,\n",
    "                                                         paragraphs=paragraphs,\n",
    "                                                         vocab=self.vocab,\n",
    "                                                         max_len=max_len))\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens,self.vocab) + (segments,is_next)) for tokens,segments,is_next in examples]\n",
    "\n",
    "        self.all_token_ids,self.all_segment,self.valid_len,self.all_pred_position,self.all_mlm_weights,self.all_mlm_label,self.nsp_labels = _pad_bert_inputs(examples=examples,max_len=max_len,vocab=self.vocab)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.all_token_ids[index],self.all_segment[index],self.valid_len[index],self.all_pred_position[index],self.all_mlm_weights[index],self.all_mlm_label[index],self.nsp_labels[index]\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wiki(batch_size,max_len):\n",
    "        num_works = d2l.get_dataloader_workers()\n",
    "        data_dir = d2l.download_extract('wikitext-2','wikitext-2')\n",
    "        paragraphs = _read_wiki(data_dir)\n",
    "        train_set = _WikiTextDataset(paragraphs,max_len)\n",
    "        train_iter = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True,num_workers=num_works)\n",
    "        return train_iter,train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79603768",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size ,max_len = 512,64\n",
    "train_iter,vocab = d2l.load_data_wiki(batch_size,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(vocab_size=len(vocab),\n",
    "                    num_hiddens=128,\n",
    "                    ffn_num_hiddens=156,num_heads=2,\n",
    "                    dropout=0.2)\n",
    "device = d2l.try_all_gpus()\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch_loss_bert(net,\n",
    "                         loss,\n",
    "                         vocab_size,\n",
    "                         token_x,\n",
    "                         segment_X,\n",
    "                         valid_len_X,\n",
    "                         pred_position_x,\n",
    "                         mlm_weights_X,\n",
    "                         mlm_Y,\n",
    "                         nsp_y):\n",
    "    _,mlm_Y_hat,nsp_Y_hat = net(token_x,segment_X,valid_len_X.reshape(-1),pred_position_x)\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1,vocab_size,mlm_Y.reshape(-1))) * mlm_weights_X.reshape(-1,1)\n",
    "    mlm_l = mlm_l.sum()/(mlm_weights_X.sum() + 1e-8)\n",
    "    nsp_l = loss(nsp_Y_hat,nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l,nsp_l,l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoGluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
