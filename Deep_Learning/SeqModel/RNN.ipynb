{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2328bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "\n",
    "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def download(name, cache_dir=os.path.join('..', 'data')):  #@save\n",
    "    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} 不存在于 {DATA_HUB}\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # 命中缓存\n",
    "    print(f'正在从{url}下载{fname}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "def read_time_machine():\n",
    "    with open(download('time_machine'),'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    return [re.sub('[^A-Za-z]+',' ',line).strip().lower() for line in lines]\n",
    "def tokenize(lines,token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误')\n",
    "class Vocab:\n",
    "    def __init__(self,tokens=None,min_freq=0,reserved_token=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_token is None:\n",
    "            reserved_token = []\n",
    "        counter = self.count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(),key=lambda x :x[1],reverse=True)\n",
    "        \n",
    "        self.unk , uniq_token = 0,['<unk>'] + reserved_token\n",
    "\n",
    "        uniq_token+=[\n",
    "            token for token,freq in self.token_freqs\n",
    "            if freq >= min_freq and token not in uniq_token\n",
    "        ] \n",
    "        self.idx_to_token ,self.token_to_idx = [],dict()\n",
    "        for token in uniq_token:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "    def count_corpus(self,tokens):\n",
    "        if len(tokens)==0 or isinstance(tokens[0],list):\n",
    "            tokens = [token for line in tokens\n",
    "                      for token in line]\n",
    "        return collections.Counter(tokens)\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,indices):\n",
    "        if not isinstance(indices,(list,tuple)):\n",
    "            return self.token_to_idx[indices]\n",
    "        return [self.token_to_idx[index] for index in indices]\n",
    "    def to_tokens(self,indices):\n",
    "        if not isinstance(indices,(list,tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "bath_size ,num_step = 32,35\n",
    "def seq_data_iter_random(corpus,bathch_size,num_step):\n",
    "    \n",
    "    corpus = corpus[random.randint(0,num_step-1):]\n",
    "    num_subseqs = (len(corpus) - 1) // num_step\n",
    "    # 保留label\n",
    "    initial_indices = list(range(0,num_subseqs * num_step,num_step))\n",
    "    random.shuffle(initial_indices)\n",
    "    \n",
    "    def data(pos):\n",
    "        return corpus[pos :pos+num_step]\n",
    "    # def label(pos):\n",
    "    #     return corpus[pos+num_step]\n",
    "    num_batches = num_subseqs // bathch_size\n",
    "    for i in range(0,bathch_size * num_batches,bathch_size):\n",
    "        initial_indices_per_batch = initial_indices[i:i+bathch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X),torch.tensor(Y)\n",
    "\n",
    "def seq_data_iter_sequential(corpus,bath_size,num_step):\n",
    "    offset = random.randint(0,num_step)\n",
    "    num_tokens = ((len(corpus) - offset - 1)//bath_size) * bath_size\n",
    "\n",
    "    Xs = torch.tensor(corpus[offset:offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "\n",
    "    Xs,Ys = Xs.reshape(bath_size ,-1),Ys.reshape(bath_size,-1)\n",
    "    num_batches = Xs.shape[-1] // num_step\n",
    "\n",
    "    for i in range(0,num_step * num_batches,num_step):\n",
    "        x = Xs[:,i:i+num_step]\n",
    "        y = Ys[:,i:i+num_step]\n",
    "        yield x,y\n",
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines,'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = [\n",
    "        vocab[token] for line in tokens\n",
    "                    for token in line \n",
    "    ]\n",
    "    if max_tokens >0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    \n",
    "    return corpus,vocab\n",
    "\n",
    "class SeqDataloader:\n",
    "    def __init__(self,batch_size,num_step,use_random_split,max_token):\n",
    "        if use_random_split:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        \n",
    "        self.corpus , self.vocab = load_corpus_time_machine(max_tokens=max_token)\n",
    "        self.batch_size,self.num_step = batch_size,num_step\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus,self.batch_size,self.num_step)\n",
    "def load_data_time_machine(batch_size,\n",
    "                           num_step,\n",
    "                           use_random_split=False,\n",
    "                           max_token=100000):\n",
    "    data_iter = SeqDataloader(\n",
    "        batch_size=batch_size,\n",
    "        num_step=num_step,\n",
    "        use_random_split=use_random_split,\n",
    "        max_token=max_token\n",
    "    )\n",
    "    return data_iter,data_iter.vocab\n",
    "\n",
    "train_iter,vocab = load_data_time_machine(batch_size=bath_size,num_step=num_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5824e5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0,2]),len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba2b6360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2,5))\n",
    "F.one_hot(X.T,28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147fc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size,num_hiddens,devices):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.rand(size=shape,device=devices) * 0.01\n",
    "    \n",
    "    W_xh = normal((num_inputs,num_hiddens))\n",
    "    W_hh = normal((num_hiddens,num_hiddens))\n",
    "\n",
    "    b_h = torch.zeros(num_hiddens,device=devices)\n",
    "\n",
    "    W_hq = normal((num_hiddens,num_outputs))\n",
    "    b_q = torch.zeros(num_outputs,device=devices)\n",
    "\n",
    "    params = [W_xh,W_hh,b_h,W_hq,b_q]\n",
    "\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "        \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3646a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_stats(bath_size,num_hidden,device):\n",
    "    return (torch.zeros((bath_size,num_hidden),device=device),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3414a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs,state,params):\n",
    "    W_xh , W_hh ,b_h,W_hq,b_q = params\n",
    "\n",
    "    H, =state\n",
    "    outputs = []\n",
    "\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X,W_xh) + torch.mm(H,W_hh) + b_h)\n",
    "        Y = torch.mm(H,W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs,dim=0),(H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f537b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch():\n",
    "    def __init__(self,vocab_size,num_hidden,device,get_params,init_state,forward_fn):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hidden\n",
    "        self.params = get_params(vocab_size,num_hidden,device)\n",
    "        self.init_state , self.forward_dn =init_state,forward_fn\n",
    "\n",
    "\n",
    "    def __call__(self,X,state):\n",
    "        X = F.one_hot(X.T,self.vocab_size).type(torch.float32)\n",
    "        return self.forward_dn(X,state,self.params)\n",
    "    \n",
    "    def begin_state(self,batch_size,device):\n",
    "        return self.init_state(batch_size,self.num_hiddens,device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "925e2fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hidden = 512 \n",
    "net = RNNModelScratch(len(vocab),num_hidden,'cuda',get_params,init_rnn_stats,rnn)\n",
    "state = net.begin_state(X.shape[0],'cuda')\n",
    "Y,new_state = net(X.to('cuda'),state)\n",
    "Y.shape,len(new_state),new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c91a7c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller vvvvvvvvvv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_ch8(prefix,num_preds,net,vocab,device):\n",
    "    state = net.begin_state(batch_size=1,device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_params = lambda:torch.tensor([outputs[-1]],device=device).reshape((1,1))\n",
    "\n",
    "    for y in prefix[1:]:\n",
    "        _,state = net(get_params(),state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):\n",
    "        y,state = net(get_params(),state)\n",
    "        outputs.append(int(torch.argmax(y,dim=1).reshape(1)))\n",
    "    \n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "predict_ch8(\"time traveller \",10,net,vocab,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "329f5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net,theta):\n",
    "    if isinstance(net,nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "158be033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "248fb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch8(net,train_iter,loss,updater,device,use_random_iter):\n",
    "    state = None\n",
    "    metric = Accumulator(2)\n",
    "    for X,Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state(batch_size=X.shape[0],device=device)\n",
    "        else:\n",
    "            if isinstance(net,nn.Module) and not isinstance(state,tuple):\n",
    "                state.detach_()\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X,y = X.to(device),y.to(device)\n",
    "        y_hat ,state = net(X,state)\n",
    "        l = loss(y_hat,y.long()).mean()\n",
    "        if isinstance(updater,torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net,1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net,1)\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(),y.numel())\n",
    "    return math.exp(metric[0] / metric[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9306a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78eb87d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time travellere the the the the the the the the the the the the \n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller and the that the the the the the the the the the \n",
      "time traveller a could some in the laig the laight the thing the\n",
      "time traveller a self hand the time traveller a self hand the ti\n",
      "time traveller s all of the wild i to the siledily flack and the\n",
      "time traveller a seemed to me in the sard the lawh the waller a \n",
      "time traveller s grew the lang the was along the ered a strange \n",
      "time traveller a shigthen i deach deach a stoudded a said the ti\n",
      "time traveller was stoud and the little people were great distin\n",
      "time traveller s were strange and in a paranex about the time tr\n",
      "time traveller have been sound of my eyes the enith on a matthe \n",
      "time traveller clunce of the stare had sumper worns said i to my\n",
      "time traveller and intellecture to mather that the time machine \n",
      "time traveller s lookly said the time traveller suinal solight o\n",
      "time traveller i want something i had happointed to be will so a\n",
      "time traveller a ving of the fright have of the laboratory getth\n",
      "time traveller the little people down the more for the condition\n",
      "time traveller a shadde of the wanes for and the latter a real t\n",
      "time traveller s in thesure with aling to me i thone in popelain\n",
      "time travellernor and secuention at they meabs afort of the time\n",
      "time traveller who had a they more irestendey i saw augent and s\n",
      "time traveller a time how to carryed my intensuifing me hairs we\n",
      "time traveller s mere her down and last the medical man of my li\n",
      "time traveller could such a flickeredark that s greend our own t\n",
      "time traveller and latilit fit i have not on t while i said the \n",
      "time traveller pired by moon at lar the ficting my mancering ver\n",
      "time traveller a dime time as and lont peor ags in wase the stom\n",
      "time travellere tie mat and ware the far fare in wast me toin ma\n",
      "time travellere the the wate the seacl in the war the pat the sa\n",
      "time travellerfere  feeringees beest aneer ineeas inet  hees ine\n",
      "time travellery and ine soulinged dexfoung heaserthenge and and \n",
      "time travelleroned and the se there the the and an he se the for\n",
      "time travellerng and in therer and in and ors and in whe mand an\n",
      "time traveller she tien and he th there in the the the the the t\n",
      "time traveller tees of ere the aion thes of the the the t ain t \n",
      "time travellerhe low the the he the bith the the the the an ihe \n",
      "time travellersi hear thi her west orig ith tha geac tha gro  ea\n",
      "time traveller was the sand ind anden werthate thase the far in \n",
      "time travellere ald ce ald fe ale he ale he ale he ale he ale he\n",
      "time travellerloun thain than thang the thar ind harld hasd thed\n",
      "time traveller the the and ine so the the le the t t and io the \n",
      "time traveller the hing tnd the and and and and and sure the sor\n",
      "time travellerend in loonningeding ofongeanging indongeedesiof o\n",
      "time traveller in thing g wt on th an in th an in the thont in t\n",
      "time travellered ito ito ioo ie  ito ied ino ito iou ie  ito iou\n",
      "time traveller to ting he the the the fof toolle then the t the \n",
      "time travellereed thitheeen thitheees ind aned ind ineesshing in\n",
      "time travellerere g  fof  fed a  fa   fo  a  for   han  fosi a d\n",
      "time travellerere then are a anee an th then the a ane afred the\n",
      "困惑度13.6\n",
      "time travellerere then are a anee an th then the a ane afred the\n",
      "travelleree a ted wrerererel and a ine a ted areere ted whe\n"
     ]
    }
   ],
   "source": [
    "def train_ch8(net,train_iter,vocab,lr,num_epochs,device,use_random_iter=False):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if isinstance(net,nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(),lr)\n",
    "    else:\n",
    "        updater = lambda batch_size : sgd(net.params,lr,batch_size) \n",
    "    predict = lambda prefix:predict_ch8(prefix,50,net,vocab,device)\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl  = train_epoch_ch8(net,train_iter,loss,updater,device,use_random_iter)\n",
    "        if(epoch + 1)%10==0:\n",
    "            print(predict('time traveller'))\n",
    "    print(f\"困惑度{ppl:.1f}\")\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))\n",
    "\n",
    "\n",
    "num_epoch ,lr =500,1\n",
    "\n",
    "train_ch8(net,train_iter,vocab,lr,num_epoch,'cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
