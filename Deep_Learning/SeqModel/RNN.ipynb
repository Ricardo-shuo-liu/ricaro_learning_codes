{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2328bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import collections\n",
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "\n",
    "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def download(name, cache_dir=os.path.join('..', 'data')):  #@save\n",
    "    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} 不存在于 {DATA_HUB}\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # 命中缓存\n",
    "    print(f'正在从{url}下载{fname}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "def read_time_machine():\n",
    "    with open(download('time_machine'),'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    return [re.sub('[^A-Za-z]+',' ',line).strip().lower() for line in lines]\n",
    "def tokenize(lines,token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误')\n",
    "class Vocab:\n",
    "    def __init__(self,tokens=None,min_freq=0,reserved_token=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_token is None:\n",
    "            reserved_token = []\n",
    "        counter = self.count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(),key=lambda x :x[1],reverse=True)\n",
    "        \n",
    "        self.unk , uniq_token = 0,['<unk>'] + reserved_token\n",
    "\n",
    "        uniq_token+=[\n",
    "            token for token,freq in self.token_freqs\n",
    "            if freq >= min_freq and token not in uniq_token\n",
    "        ] \n",
    "        self.idx_to_token ,self.token_to_idx = [],dict()\n",
    "        for token in uniq_token:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "    def count_corpus(self,tokens):\n",
    "        if len(tokens)==0 or isinstance(tokens[0],list):\n",
    "            tokens = [token for line in tokens\n",
    "                      for token in line]\n",
    "        return collections.Counter(tokens)\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,indices):\n",
    "        if not isinstance(indices,(list,tuple)):\n",
    "            return self.token_to_idx[indices]\n",
    "        return [self.token_to_idx[index] for index in indices]\n",
    "    def to_tokens(self,indices):\n",
    "        if not isinstance(indices,(list,tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "bath_size ,num_step = 32,35\n",
    "def seq_data_iter_random(corpus,bathch_size,num_step):\n",
    "    \n",
    "    corpus = corpus[random.randint(0,num_step-1):]\n",
    "    num_subseqs = (len(corpus) - 1) // num_step\n",
    "    # 保留label\n",
    "    initial_indices = list(range(0,num_subseqs * num_step,num_step))\n",
    "    random.shuffle(initial_indices)\n",
    "    \n",
    "    def data(pos):\n",
    "        return corpus[pos :pos+num_step]\n",
    "    # def label(pos):\n",
    "    #     return corpus[pos+num_step]\n",
    "    num_batches = num_subseqs // bathch_size\n",
    "    for i in range(0,bathch_size * num_batches,bathch_size):\n",
    "        initial_indices_per_batch = initial_indices[i:i+bathch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X),torch.tensor(Y)\n",
    "\n",
    "def seq_data_iter_sequential(corpus,bath_size,num_step):\n",
    "    offset = random.randint(0,num_step)\n",
    "    num_tokens = ((len(corpus) - offset - 1)//bath_size) * bath_size\n",
    "\n",
    "    Xs = torch.tensor(corpus[offset:offset + num_tokens])\n",
    "    Ys = torch.tensor([corpus[offset + 1: offset + 1 + num_tokens]])\n",
    "\n",
    "    Xs,Ys = Xs.reshape(bath_size ,-1),Ys.reshape(bath_size,-1)\n",
    "    num_batches = Xs.shape[-1] // num_step\n",
    "\n",
    "    for i in range(0,num_step * num_batches,num_step):\n",
    "        x = Xs[:,i:i+num_step]\n",
    "        y = Ys[:,i:i+num_step]\n",
    "        yield x,y\n",
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines,'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = [\n",
    "        vocab[token] for line in tokens\n",
    "                    for token in line \n",
    "    ]\n",
    "    if max_tokens >0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    \n",
    "    return corpus,vocab\n",
    "\n",
    "class SeqDataloader:\n",
    "    def __init__(self,batch_size,num_step,use_random_split,max_token):\n",
    "        if use_random_split:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        \n",
    "        self.corpus , self.vocab = load_corpus_time_machine(max_tokens=max_token)\n",
    "        self.batch_size,self.num_step = batch_size,num_step\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus,self.batch_size,self.num_step)\n",
    "def load_data_time_machine(batch_size,\n",
    "                           num_step,\n",
    "                           use_random_split=False,\n",
    "                           max_token=1000):\n",
    "    data_iter = SeqDataloader(\n",
    "        batch_size=batch_size,\n",
    "        num_step=num_step,\n",
    "        use_random_split=use_random_split,\n",
    "        max_token=max_token\n",
    "    )\n",
    "    return data_iter,data_iter.vocab\n",
    "\n",
    "train_iter,vocab = load_data_time_machine(batch_size=bath_size,num_step=num_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5824e5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0,2]),len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2b6360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2,5))\n",
    "F.one_hot(X.T,28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147fc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size,num_hiddens,devices):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.rand(size=shape,device=devices) * 0.01\n",
    "    \n",
    "    W_xh = normal((num_inputs,num_hiddens))\n",
    "    W_hh = normal((num_hiddens,num_hiddens))\n",
    "\n",
    "    b_h = torch.zeros(num_hiddens,device=devices)\n",
    "\n",
    "    W_hq = normal((num_hiddens,num_outputs))\n",
    "    b_q = torch.zeros(num_outputs,device=devices)\n",
    "\n",
    "    params = [W_xh,W_hh,b_h,W_hq,b_q]\n",
    "\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_stats(bath_size,num_hidden,device):\n",
    "    return (torch.zeros((bath_size,num_hidden),device=device),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs,state,params):\n",
    "    W_xh , W_hh ,b_h,W_hq,b_q = params\n",
    "\n",
    "    H, =state\n",
    "    outputs = []\n",
    "\n",
    "    for X in input:\n",
    "        H = torch.tanh(torch.mm(X,W_xh) + torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
